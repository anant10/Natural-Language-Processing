{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NULabTMN/ps2-AnantShanbhag93/blob/master/LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "moWB9udaKesP"
      },
      "source": [
        "Your task is to train *character-level* language models. \n",
        "You will train unigram, bigram, and trigram character level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gHFJmuftHJld",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import httpimport\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8U0UCuyHQkai"
      },
      "source": [
        "This code loads the training and test data. Each dataset is a list of books. Each book contains a list of sentences, and each sentence contains a list of words. For building a character language model, you should join the words of a sentence together with a space character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6x0pfuiEChTh",
        "colab": {}
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_WAO9VjFLArq"
      },
      "source": [
        "## 1.1\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n",
        "\n",
        "If your machine takes a long time to perform this computation, you may save these counts to files in your github repository and load them on request. This is not necessary, however."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeyfCvQm3wis",
        "colab_type": "code",
        "colab": {},
        "outputId": "495c8c0c-1e5d-49fb-cecc-0901dc0def3c"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "# print(train[0])\n",
        "all_sent = []\n",
        "for each_book in train:\n",
        "    for each_sentence in each_book:\n",
        "        all_sent.append(\" \".join(each_sentence))\n",
        "\n",
        "print(len(all_sent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg8FlkUF3wiv",
        "colab_type": "code",
        "colab": {},
        "outputId": "bc4b75b6-4a28-4753-fb69-c38443f404dc"
      },
      "source": [
        "print(all_sent[:3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkawhaQD3wiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def get_uni_stat(listOfSentences):\n",
        "    vect = CountVectorizer(ngram_range=(1,1), analyzer='char', lowercase=False)\n",
        "    out = vect.fit_transform(listOfSentences).toarray()\n",
        "    count_ngrams = out.sum(axis=0)\n",
        "    vocab = vect.get_feature_names()\n",
        "    stat_uni = dict(zip(vocab, count_ngrams))\n",
        "    return stat_uni\n",
        "#     total_count_uni = count_ngrams.sum(axis=0)\n",
        "\n",
        "def get_bi_stat(listOfSentences):\n",
        "    vect = CountVectorizer(ngram_range=(2,2), analyzer='char', lowercase=False)\n",
        "    out = vect.fit_transform(listOfSentences).toarray()\n",
        "    count_ngrams = out.sum(axis=0)\n",
        "    vocab = vect.get_feature_names()\n",
        "    stat_bi = dict(zip(vocab, count_ngrams))\n",
        "    return stat_bi\n",
        "\n",
        "def get_tri_stat(listOfSentences):\n",
        "    vect = CountVectorizer(ngram_range=(3,3), analyzer='char', lowercase=False)\n",
        "    out = vect.fit_transform(listOfSentences).toarray()\n",
        "    count_ngrams = out.sum(axis=0)\n",
        "    vocab = vect.get_feature_names()\n",
        "    stat_tri = dict(zip(vocab, count_ngrams))\n",
        "    return stat_tri\n",
        "#     total_count_tri = count_ngrams.sum(axis=0)\n",
        "# print(stat_uni)\n",
        "# print(stat_bi)\n",
        "# print(stat_tri)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69a66e273wi1",
        "colab_type": "code",
        "colab": {},
        "outputId": "6e6c20fe-5553-4310-a1cc-ed31c9062fcc"
      },
      "source": [
        "stat_uni = get_uni_stat(all_sent)\n",
        "print(len(stat_uni))\n",
        "total_count_uni=(sum(stat_uni.values()))\n",
        "print(total_count_uni)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95\n",
            "12010505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLACfOjY3wi3",
        "colab_type": "code",
        "colab": {},
        "outputId": "04cd8b0a-ec37-40bb-fdbc-9b29be8c5cd4"
      },
      "source": [
        "stat_bi = get_bi_stat(all_sent)\n",
        "print(len(stat_bi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUnJtd4A3wi5",
        "colab_type": "code",
        "colab": {},
        "outputId": "9132c789-6fd9-468d-ed45-4c2d3eb67468"
      },
      "source": [
        "stat_tri = get_tri_stat(all_sent)\n",
        "print(len(stat_tri))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RS3mnaIvQnhI"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using linear interpolation smoothing method. For determining λs for linear interpolation, you can divide the training data into a new training set (80%) and a held-out set (20%), then using grid search method:\n",
        "Choose ~10 values of λ to test using grid search on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
        "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvifPSs_3wi9",
        "colab_type": "code",
        "colab": {},
        "outputId": "cb9d4725-c81b-400e-ec81-30c3b5c9cf05"
      },
      "source": [
        "import math\n",
        "length = math.ceil(len(all_sent)*.8)\n",
        "#print(length)\n",
        "training = all_sent[:length]\n",
        "held_out = all_sent[length:]\n",
        "held_out_seq = \" \".join(held_out)\n",
        "print(len(all_sent))\n",
        "print(len(training))\n",
        "print(len(held_out))\n",
        "# print(held_out_seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "98552\n",
            "78842\n",
            "19710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo7lj7ep3wi_",
        "colab_type": "code",
        "colab": {},
        "outputId": "f33cc2c0-8694-4c0d-cfa7-86d934f8dab9"
      },
      "source": [
        "stath_uni = get_uni_stat(training)\n",
        "print(len(stath_uni))\n",
        "total_count_uni_h=(sum(stath_uni.values()))\n",
        "print(total_count_uni_h)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92\n",
            "9594734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhpGKl7G3wjB",
        "colab_type": "code",
        "colab": {},
        "outputId": "d3104315-f5ba-4b99-89ec-23e3ee858656"
      },
      "source": [
        "stath_bi = get_bi_stat(training)\n",
        "print(len(stath_bi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkpyj3N93wjD",
        "colab_type": "code",
        "colab": {},
        "outputId": "6b6bef1b-b26f-4e26-a9fb-fbed2d8ffc49"
      },
      "source": [
        "stath_tri = get_tri_stat(training)\n",
        "print(len(stath_tri))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmPxnqaK3wjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prob_uni_h(w):\n",
        "    return stath_uni.get(w, 50)/total_count_uni_h\n",
        "def get_prob_bi_h(v, w):\n",
        "    return stath_bi.get(v+w, 0)/stath_uni.get(v, 2)\n",
        "def get_prob_tri_h(u, v, w):\n",
        "    return stath_tri.get(u+v+w, 0)/stath_bi.get(u+v, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lifxwl_K3wjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prob_uni(w):\n",
        "    return stat_uni.get(w, 50)/total_count_uni\n",
        "def get_prob_bi(v, w):\n",
        "    return stat_bi.get(v+w, 0)/stat_uni.get(v, 2)\n",
        "def get_prob_tri(u, v, w):\n",
        "    return stat_tri.get(u+v+w, 0)/stat_bi.get(u+v, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWqZPNOc3wjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lam =  [[0.333, 0.333, 0.333],\n",
        "        [0.1, 0.1, 0.8],\n",
        "        [0.1, 0.8, 0.1],\n",
        "        [0.8, 0.1, 0.1],\n",
        "        [0.2, 0.2, 0.6],\n",
        "        [0.2, 0.6, 0.2],\n",
        "        [0.6, 0.2, 0.2],\n",
        "        [0.25, 0.25, 0.5],\n",
        "        [0.25, 0.5, 0.25],\n",
        "        [0.5, 0.25, 0.25],\n",
        "        [0.1, 0.3, 0.6],\n",
        "        [0.15, 0.3, 0.6],\n",
        "        [0.1, 0.4, 0.5]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpZjd0ql3wjN",
        "colab_type": "code",
        "colab": {},
        "outputId": "d0464f7c-25fe-462b-b272-e28ebc605ff6"
      },
      "source": [
        "import numpy as np\n",
        "def get_perplexity_for_doc(m, doc, lambda1, lambda2, lambda3, get_prob1, get_prob2, get_prob3):\n",
        "    ent = 0\n",
        "    for i in range(0, len(doc)-2):\n",
        "        u = doc[i]\n",
        "        v = doc[i+1]\n",
        "        w = doc[i+2]\n",
        "        prob = (l1 * get_prob3(u,v,w)) + (l2 * get_prob2(v, w)) + (l1 * get_prob1(w))\n",
        "        ent = ent + np.log2(prob)\n",
        "    ent =  (1/m) *  ent\n",
        "    perp = 2 ** (-ent)\n",
        "    return perp\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "import math\n",
        "Mlen = len(held_out_seq)\n",
        "list_perplexities = []\n",
        "for l in lam:\n",
        "    l1 = l[0]\n",
        "    l2 = l[1] \n",
        "    l3 = l[2]\n",
        "   # print(l1, l2, l3)\n",
        "    list_perplexities.append(get_perplexity_for_doc(Mlen, held_out_seq, l1, l2, l3, get_prob_uni_h, get_prob_bi_h, get_prob_tri_h ))\n",
        "print(list_perplexities)\n",
        "min_index = list_perplexities.index(min(list_perplexities))\n",
        "print(min_index)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10.06132267740397, 33.504171418046404, 10.578923491445957, 5.971485875782114, 16.75209524441831, 10.22933310114893, 7.204841368635145, 13.401678651400935, 10.130259399278184, 8.048997844539398, 20.458654556985515, 16.883758582979464, 17.200614595895775]\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kNUqRUY3wjR",
        "colab_type": "code",
        "colab": {},
        "outputId": "0ff0f50b-6e91-421c-873a-b4995262af55"
      },
      "source": [
        "lambda_selected = lam[min_index]\n",
        "print(lambda_selected)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8, 0.1, 0.1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia57qivN3wjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "all_test = []\n",
        "for test_document in test:\n",
        "    each_doc = []\n",
        "    for each_sent in test_document:\n",
        "        t = \" \".join(each_sent)\n",
        "        each_doc.append(t)\n",
        "    all_test.append(\" \".join(each_doc))\n",
        "#     t = \" \".join(test_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haNHWbkk3wjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data_for_df = {\"document\":all_test, \"file\":test_files, \"lang\":\"\", \"perp\":np.nan,\"correctness\":False}\n",
        "df = pd.DataFrame(data_for_df)\n",
        "for i in range(len(df)):\n",
        "    if df[\"file\"].iloc[i].endswith('.txt'):\n",
        "        df['lang'].iloc[i] = 'por'\n",
        "    else:\n",
        "        df['lang'].iloc[i] = 'eng'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruwsC6UF3wjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 220):\n",
        "    each_doc = df[\"document\"].iloc[i]\n",
        "    ml = len(each_doc)\n",
        "    df[\"perp\"].iloc[i] = get_perplexity_for_doc(ml, each_doc, lambda_selected[0], lambda_selected[1],lambda_selected[2], get_prob_uni, get_prob_bi, get_prob_tri )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqvwPJq73wja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 33\n",
        "for i in range(len(df)):\n",
        "    if df[\"lang\"].iloc[i] == 'por' and df[\"perp\"].iloc[i] >= threshold:\n",
        "        df['correctness'].iloc[i] = True\n",
        "    elif df[\"lang\"].iloc[i] == 'eng' and df[\"perp\"].iloc[i] < threshold:\n",
        "        df['correctness'].iloc[i] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SCD6HCg3wjc",
        "colab_type": "code",
        "colab": {},
        "outputId": "7568cf23-0d96-4dc6-98c3-5e1123b49163"
      },
      "source": [
        "df2 = df.copy()\n",
        "df2 = df2.sort_values(by=['perp'])\n",
        "print(df2[170:220])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              document          file lang  \\\n",
            "50   ( Do start fires one or two hours ahead of tim...          ce14  eng   \n",
            "35   If the Cardinals heed Manager Gene Mauch of th...          ca15  eng   \n",
            "132  The presidency : talking and listening Though ...          ca42  eng   \n",
            "82   The most surprising thing about the Twenty-sec...          cb25  eng   \n",
            "68   Too often a beginning bodybuilder has to do hi...          ce01  eng   \n",
            "142  The nation the three-front war At a closed-doo...          ca37  eng   \n",
            "114  To hold a herd of cattle on a new range till t...          cf35  eng   \n",
            "124  Rookie Ron Nischwitz continued his pinpoint pi...          ca13  eng   \n",
            "14   When Mickey Charles Mantle , the New York Yank...          ca39  eng   \n",
            "59   Resentment welled up yesterday among Democrati...          ca07  eng   \n",
            "69   About 70 North Providence taxpayers made appea...          ca24  eng   \n",
            "178  Knowing specifically what the many feed additi...          ce27  eng   \n",
            "126  Greer Garson , world-famous star of stage , sc...          ca29  eng   \n",
            "10   City Controller Alexander Hemphill charged Tue...          ca09  eng   \n",
            "137  Austin , Texas -- Committee approval of Gov. P...          ca02  eng   \n",
            "212  The design of a mechanical interlocking frame ...          ce07  eng   \n",
            "100  California Democrats this weekend will take th...          cb11  eng   \n",
            "141  Asilomar , March 26 Vast spraying programs con...          ca25  eng   \n",
            "92   Into Washington on President-elect John F. Ken...          ca40  eng   \n",
            "58   The average reader of this magazine owns more ...          ce10  eng   \n",
            "36   Austin , Texas -- A Texas halfback who doesn't...          ca12  eng   \n",
            "120  As autumn starts its annual sweep , few Americ...          cc15  eng   \n",
            "208  After being closed for seven months , the Gard...          ca17  eng   \n",
            "11   Hotel Escape's Bonanza room has a real bonanza...          ca31  eng   \n",
            "106  Miami , Fla. , March 17 -- The Orioles tonight...          ca11  eng   \n",
            "91   Santa Barbara -- `` The present recovery movem...          ca27  eng   \n",
            "65   Salem ( special ) -- For a second month in a r...          ca23  eng   \n",
            "127  Romantic news concerns Mrs. Joan Monroe Armour...          ca16  eng   \n",
            "161  `` A Night in New Orleans '' is the gayety pla...          ca18  eng   \n",
            "30   Orlando , Fla. , Feb. 2 -- The best 2-year-old...          ce09  eng   \n",
            "117  Centro Nacional de Pesquisa de Soja ( CNPSo ) ...   ag94fe1.txt  por   \n",
            "90   Os sojicultores brasileiros invadiram o Paragu...  ag94ja11.txt  por   \n",
            "209  Pelo menos cinco candidatos de o PMDB a govern...  br94ab02.txt  por   \n",
            "183  O fim melancólico de a revisão constitucional ...  br94ju01.txt  por   \n",
            "218  Covas apressou se em anunciar anteontem três s...  br94de01.txt  por   \n",
            "181  Com US$ 300 mensais é possível manter um caval...   ag94mr1.txt  por   \n",
            "43   brasil Críticas de sobra ; Mantendo distância ...  br94ma01.txt  por   \n",
            "143  A primeira avaliação científica de os reflexos...  br94jl01.txt  por   \n",
            "200  Criadores brasileiros fazem remate em a Bolívi...  ag94ou04.txt  por   \n",
            "139  Agricultores gaúchos , paranaenses , mineiros ...  ag94de06.txt  por   \n",
            "4    Exposição e pregão de andaluz exibem cerca de ...  ag94jl12.txt  por   \n",
            "131  Costa Sena foi um de os grandes nomes de a pol...  br94ja04.txt  por   \n",
            "163  Peões boiadeiros montam atrás de prêmios em a ...  ag94ag02.txt  por   \n",
            "87   Exposição Nacional do Cavalo Árabe começa sext...  ag94no01.txt  por   \n",
            "172  Um de os momentos mais tensos de a CPI de o Co...   br94fe1.txt  por   \n",
            "18   Jersei atinge média de Cr$ 1,4 milhão em a ven...  ag94ab12.txt  por   \n",
            "80   gril Associação quer transformar produto em co...  ag94ma03.txt  por   \n",
            "145  Coluna \" Moeda Verde \" traz as relações de tro...  ag94ju07.txt  por   \n",
            "74   Chamava a atenção , ontem , o abatimento de o ...  br94ag01.txt  por   \n",
            "112  Três remates de QM faturam R$ 720 mil com vend...  ag94se06.txt  por   \n",
            "\n",
            "          perp  correctness  \n",
            "50   20.846970         True  \n",
            "35   20.849427         True  \n",
            "132  20.858945         True  \n",
            "82   20.880701         True  \n",
            "68   20.889165         True  \n",
            "142  20.960771         True  \n",
            "114  21.027519         True  \n",
            "124  21.035816         True  \n",
            "14   21.063518         True  \n",
            "59   21.097989         True  \n",
            "69   21.147024         True  \n",
            "178  21.147992         True  \n",
            "126  21.235315         True  \n",
            "10   21.385623         True  \n",
            "137  21.465236         True  \n",
            "212  21.707146         True  \n",
            "100  21.722473         True  \n",
            "141  21.795368         True  \n",
            "92   21.822028         True  \n",
            "58   21.859534         True  \n",
            "36   21.859646         True  \n",
            "120  21.893055         True  \n",
            "208  21.938320         True  \n",
            "11   22.001649         True  \n",
            "106  22.009749         True  \n",
            "91   22.213528         True  \n",
            "65   22.304712         True  \n",
            "127  22.412901         True  \n",
            "161  22.537387         True  \n",
            "30   26.643180         True  \n",
            "117  39.054572         True  \n",
            "90   39.524670         True  \n",
            "209  39.831642         True  \n",
            "183  40.092198         True  \n",
            "218  40.319919         True  \n",
            "181  40.569515         True  \n",
            "43   40.697800         True  \n",
            "143  40.722855         True  \n",
            "200  41.043139         True  \n",
            "139  41.134165         True  \n",
            "4    41.278311         True  \n",
            "131  41.298136         True  \n",
            "163  41.309925         True  \n",
            "87   41.316057         True  \n",
            "172  41.937007         True  \n",
            "18   42.109170         True  \n",
            "80   42.144272         True  \n",
            "145  42.341986         True  \n",
            "74   42.465996         True  \n",
            "112  43.239136         True  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aQl2u_giVW5e"
      },
      "source": [
        "## 1.3\n",
        "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QQF4HhQGOZD8",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "def get_smooth_perplexity(doc,m):\n",
        "    lambda_k = 0.1\n",
        "    ent = 0\n",
        "    for i in range(0, len(doc)-2):\n",
        "        u = doc[i]\n",
        "        v = doc[i+1]\n",
        "        w = doc[i+2]\n",
        "        prob = (stat_tri.get(u+v+w,0)+lambda_k)/((stat_bi.get(u+v,0)+(lambda_k*len(stat_tri))))\n",
        "        ent = ent + np.log2(prob)\n",
        "    ent =  (1/m) *  ent\n",
        "    perp = 2 ** (-ent)\n",
        "    return perp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IGUTEk8QUehL",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "for i in range(0, 220):\n",
        "    each_doc = df[\"document\"].iloc[i]\n",
        "    ml = len(each_doc)\n",
        "    df[\"perp\"].iloc[i] = get_smooth_perplexity(each_doc, ml)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zmTYesv3wji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 40\n",
        "for i in range(len(df)):\n",
        "    if df[\"lang\"].iloc[i] == 'por' and df[\"perp\"].iloc[i] >= threshold:\n",
        "        df['correctness'].iloc[i] = True\n",
        "    elif df[\"lang\"].iloc[i] == 'eng' and df[\"perp\"].iloc[i] < threshold:\n",
        "        df['correctness'].iloc[i] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2HEps-A3wjk",
        "colab_type": "code",
        "colab": {},
        "outputId": "568cf8ce-c1ef-4225-f2ad-6b968a2ba15b"
      },
      "source": [
        "df2 = df.copy()\n",
        "df2 = df2.sort_values(by=['perp'])\n",
        "print(df2[170:220])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                              document          file lang  \\\n",
            "134  Philadelphia , Jan. 23 -- Nick Skorich , the l...          ca14  eng   \n",
            "84   Mischa Elman shared last night's Lewisohn Stad...          cc11  eng   \n",
            "68   Too often a beginning bodybuilder has to do hi...          ce01  eng   \n",
            "33   St. Johns , Mich. , April 19 . -- A jury of se...          ca21  eng   \n",
            "178  Knowing specifically what the many feed additi...          ce27  eng   \n",
            "137  Austin , Texas -- Committee approval of Gov. P...          ca02  eng   \n",
            "82   The most surprising thing about the Twenty-sec...          cb25  eng   \n",
            "69   About 70 North Providence taxpayers made appea...          ca24  eng   \n",
            "132  The presidency : talking and listening Though ...          ca42  eng   \n",
            "10   City Controller Alexander Hemphill charged Tue...          ca09  eng   \n",
            "100  California Democrats this weekend will take th...          cb11  eng   \n",
            "126  Greer Garson , world-famous star of stage , sc...          ca29  eng   \n",
            "35   If the Cardinals heed Manager Gene Mauch of th...          ca15  eng   \n",
            "124  Rookie Ron Nischwitz continued his pinpoint pi...          ca13  eng   \n",
            "92   Into Washington on President-elect John F. Ken...          ca40  eng   \n",
            "212  The design of a mechanical interlocking frame ...          ce07  eng   \n",
            "14   When Mickey Charles Mantle , the New York Yank...          ca39  eng   \n",
            "141  Asilomar , March 26 Vast spraying programs con...          ca25  eng   \n",
            "11   Hotel Escape's Bonanza room has a real bonanza...          ca31  eng   \n",
            "120  As autumn starts its annual sweep , few Americ...          cc15  eng   \n",
            "58   The average reader of this magazine owns more ...          ce10  eng   \n",
            "91   Santa Barbara -- `` The present recovery movem...          ca27  eng   \n",
            "65   Salem ( special ) -- For a second month in a r...          ca23  eng   \n",
            "36   Austin , Texas -- A Texas halfback who doesn't...          ca12  eng   \n",
            "208  After being closed for seven months , the Gard...          ca17  eng   \n",
            "106  Miami , Fla. , March 17 -- The Orioles tonight...          ca11  eng   \n",
            "114  To hold a herd of cattle on a new range till t...          cf35  eng   \n",
            "127  Romantic news concerns Mrs. Joan Monroe Armour...          ca16  eng   \n",
            "161  `` A Night in New Orleans '' is the gayety pla...          ca18  eng   \n",
            "30   Orlando , Fla. , Feb. 2 -- The best 2-year-old...          ce09  eng   \n",
            "117  Centro Nacional de Pesquisa de Soja ( CNPSo ) ...   ag94fe1.txt  por   \n",
            "209  Pelo menos cinco candidatos de o PMDB a govern...  br94ab02.txt  por   \n",
            "218  Covas apressou se em anunciar anteontem três s...  br94de01.txt  por   \n",
            "90   Os sojicultores brasileiros invadiram o Paragu...  ag94ja11.txt  por   \n",
            "183  O fim melancólico de a revisão constitucional ...  br94ju01.txt  por   \n",
            "143  A primeira avaliação científica de os reflexos...  br94jl01.txt  por   \n",
            "131  Costa Sena foi um de os grandes nomes de a pol...  br94ja04.txt  por   \n",
            "172  Um de os momentos mais tensos de a CPI de o Co...   br94fe1.txt  por   \n",
            "200  Criadores brasileiros fazem remate em a Bolívi...  ag94ou04.txt  por   \n",
            "43   brasil Críticas de sobra ; Mantendo distância ...  br94ma01.txt  por   \n",
            "87   Exposição Nacional do Cavalo Árabe começa sext...  ag94no01.txt  por   \n",
            "4    Exposição e pregão de andaluz exibem cerca de ...  ag94jl12.txt  por   \n",
            "181  Com US$ 300 mensais é possível manter um caval...   ag94mr1.txt  por   \n",
            "139  Agricultores gaúchos , paranaenses , mineiros ...  ag94de06.txt  por   \n",
            "18   Jersei atinge média de Cr$ 1,4 milhão em a ven...  ag94ab12.txt  por   \n",
            "163  Peões boiadeiros montam atrás de prêmios em a ...  ag94ag02.txt  por   \n",
            "74   Chamava a atenção , ontem , o abatimento de o ...  br94ag01.txt  por   \n",
            "80   gril Associação quer transformar produto em co...  ag94ma03.txt  por   \n",
            "145  Coluna \" Moeda Verde \" traz as relações de tro...  ag94ju07.txt  por   \n",
            "112  Três remates de QM faturam R$ 720 mil com vend...  ag94se06.txt  por   \n",
            "\n",
            "          perp  correctness  \n",
            "134  13.477552         True  \n",
            "84   13.478097         True  \n",
            "68   13.556825         True  \n",
            "33   13.572581         True  \n",
            "178  13.651373         True  \n",
            "137  13.839235         True  \n",
            "82   13.871761         True  \n",
            "69   13.942452         True  \n",
            "132  13.957774         True  \n",
            "10   13.991128         True  \n",
            "100  14.049285         True  \n",
            "126  14.380558         True  \n",
            "35   14.408557         True  \n",
            "124  14.519966         True  \n",
            "92   14.550601         True  \n",
            "212  14.689981         True  \n",
            "14   14.859526         True  \n",
            "141  15.127248         True  \n",
            "11   15.203427         True  \n",
            "120  15.270067         True  \n",
            "58   15.446362         True  \n",
            "91   15.480408         True  \n",
            "65   15.530213         True  \n",
            "36   15.700292         True  \n",
            "208  15.908816         True  \n",
            "106  15.966256         True  \n",
            "114  16.278324         True  \n",
            "127  16.395054         True  \n",
            "161  17.451258         True  \n",
            "30   23.890384         True  \n",
            "117  48.647437         True  \n",
            "209  48.703733         True  \n",
            "218  49.405102         True  \n",
            "90   49.715634         True  \n",
            "183  49.838642         True  \n",
            "143  50.208757         True  \n",
            "131  51.199275         True  \n",
            "172  52.198256         True  \n",
            "200  52.495390         True  \n",
            "43   52.602515         True  \n",
            "87   52.751342         True  \n",
            "4    53.149571         True  \n",
            "181  53.298789         True  \n",
            "139  53.439174         True  \n",
            "18   53.699307         True  \n",
            "163  53.718902         True  \n",
            "74   53.764259         True  \n",
            "80   55.447291         True  \n",
            "145  55.655224         True  \n",
            "112  58.221120         True  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bqhXTB5TXR25"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tFq1ECgDI6QG"
      },
      "source": [
        "* The trigram model conditions on lot of context, i.e. given sufficient training data the counts for most of the trigrams are high and therefore tries to converge to the true value and the model has low bias. Where as unigram model completely ignores the context and tries to converge to inferior value and has high bias. A bigram model even though considers a bit of context falls between trigram and unigram models showing relatively high bias than the trigram model.\n",
        "\n",
        "* In linear interpolation method, trigram model is given higher weightage than bigram and unigram. The disadvantage of trigram is that many times the trigrams in test is not present in the training set and we assign 0 to trigram value. Same for bigram value. In such conditions the perplexity is directly dependent on the unigram model which uses UNK feature to give some value to the unseen characters. As the unigram model has high bias , it will try to influence the linear interpolation. Because of this the margin between English and portugese articles is comparatively lesser than the margin between the same when calculated using trigram model with lambda smoothing. Even if the trigrams are present in test, the inclusion of bigram and unigram models in the linear interpolation induces a comparatively high bias.\n",
        "\n",
        "* In Lambda smoothing we consider only trigram model. As mentioned earlier trigram model is conditioned on the context had has low bias and converges to true value. The disadvatage of not having trigrams is training set due to insufficient training data induces a relatively high variance. This issue is handled using lambda smoothing without depending on inferior models like unigram and bigram. So the trigram model with lambda smoothing has lower bias than the linear interpolation method and is capable of distringuishing between english and portugese atricles with a bigger margin of perplexity scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XISS4uyL3wjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}